{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implementing Value Iteration to Find Optimal $\\pi'$ for a Karel Task $T_2$\n",
                "# Documentation\n",
                "There are multiple classes used to implement value iteration to find the optimal policy $\\pi$. \n",
                "## bcoolors\n",
                "This class is only useful for the output map coloring\n",
                "## Direction\n",
                "An enum structer specifying directions\n",
                "## Action\n",
                "An enum structer specifiyng possible actions\n",
                "## ENV \n",
                "The class where all environment-related details are stored.\n",
                "## MDP\n",
                "This class contains all MDP-related values, implements value iteration and executes the reached policy.\n",
                "\n",
                "The last two will be expanded on before their respective cells."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Importing Necessary libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 206,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import time\n",
                "from enum import Enum\n",
                "from os import system, name\n",
                "from time import sleep\n",
                "from IPython.display import clear_output\n",
                "from ast import literal_eval as make_tuple\n",
                "import random\n",
                "import time\n",
                "# seed random number generator\n",
                "random.seed(73)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 207,
            "metadata": {},
            "outputs": [],
            "source": [
                "class bcolors:\n",
                "    HEADER = '\\033[95m'\n",
                "    OKBLUE = '\\033[94m'\n",
                "    OKCYAN = '\\033[96m'\n",
                "    OKGREEN = '\\033[92m'\n",
                "    WARNING = '\\033[93m'\n",
                "    FAIL = '\\033[91m'\n",
                "    ENDC = '\\033[0m'\n",
                "    BOLD = '\\033[1m'\n",
                "    UNDERLINE = '\\033[4m'\n",
                "\n",
                "class Direction(Enum):\n",
                "    UP = 0\n",
                "    RIGHT = 1\n",
                "    DOWN = 2\n",
                "    LEFT = 3\n",
                "    \n",
                "class Action(Enum):\n",
                "    move = 0 \n",
                "    turnLeft = 1 \n",
                "    turnRight = 2 \n",
                "    finish = 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Environemnt Class (ENV)\n",
                "This class contains all environment-relataed details in a paramettrized manner. \n",
                "## ENV Attribues:\n",
                "- `W` (int): width of the map\n",
                "- `H`(int): height of the map\n",
                "- `walls` list(): a list containing all tuple positions of the walls\n",
                "- `avatar_looks` list(): contains different orientations looks indexed in-order with the `Direction` enum class.\n",
                "- `map` list(list()): contains the current Karel configuragion based on the AVATAR's state.\n",
                "## ENV Methods\n",
                "- `make_env(self, state)`: takes the state of the avatar and generates the environemnt accordingly, then stores it in `self.map`.\n",
                "- `show(self, state)`: prints the current map given the AVATAR's state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 208,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ENV():\n",
                "    def __init__(self, W=4, H=4, walls=[(1, 2), (2, 3)]):\n",
                "        self.W = W\n",
                "        self.H = H\n",
                "        self.walls = walls\n",
                "        self.avatar_looks = ['^', '>', 'v', '<']\n",
                "    def make_env(self, State):\n",
                "        self.map = [['.' for i in range(self.W)] for j in range(self.H)]\n",
                "        for wall in self.walls:\n",
                "            self.map[wall[0]][wall[1]] = '#'\n",
                "        self.map[State[0]][State[1]] = self.avatar_looks[State[2]]\n",
                "    def show(self, State):\n",
                "        self.make_env(State)\n",
                "        for r in range(self.W):\n",
                "            for c in range(self.H):\n",
                "                if self.map[r][c] in self.avatar_looks:\n",
                "                    print(bcolors.OKBLUE + self.map[r][c] + bcolors.ENDC,\" \", end='')\n",
                "                elif self.map[r][c] == 'm':\n",
                "                    print(bcolors.WARNING + 'm' + bcolors.ENDC,\" \", end='')\n",
                "                elif self.map[r][c] == '#':\n",
                "                    print(bcolors.FAIL + '#' + bcolors.ENDC,\" \", end='') \n",
                "                else:\n",
                "                    print(self.map[r][c], \" \", end='')\n",
                "            print('\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Makrov-Decision Process Class (MDP)\n",
                "This class contain all MDP-related dynamics.\n",
                "## MDP Attributes\n",
                "- `initial_state`: a 3-tuple containg the intial state of task $T_1$, which is `(1, 2, Direction.LEFT.value)`.\n",
                "- `final_state`: a 3-tuple represeing the post-grid configuration, which is `(3, 2, Direction.RIGHT.value)`\n",
                "- `env`: an `ENV()` class instance\n",
                "- `S`: a list of 3-tuples containing all possible states, $(i, j, dir)$ such that:  \n",
                "\n",
                ">>> $0 \\leq i \\leq HEIGHT -1$  \n",
                "\n",
                ">>> $0 \\leq j \\leq WIDTH - 1 $  \n",
                "\n",
                ">>> $dir \\in \\{UP, RIGHT, DOWN, LEFT\\}$\n",
                "\n",
                ">>> it also include the terminal state, namely, `terminal`\n",
                "\n",
                "- `A`: a dictionary of $S$, which contains all possible actions given $S$. This is $A(S)$\n",
                "- `V`: a dictionary of $S$, which represent the value function of $S$. This is $V(S)$\n",
                "## MDP Methods\n",
                "- `generateState(self):` generates all possible states and assigns them to the attribute `S`.\n",
                "- `generateActions(self):` generates $A(S)$ and assigns them to $A$.\n",
                "- ` generateV(self)`: $\\forall_{s\\in S} V(S) \\gets 0$\n",
                "- `reward(s, a)`: takes $s$ and $a$, returns $100$ if the state is the desired post-grid and $a=finish$. Returns -1, otherwise.\n",
                "- `p(s, a)`: represents the environment dynamics which takes $s$ and $a$ and returns the next state $s'$ and immediate reward $r$.\n",
                "- `ValueIteration(theta, V, S, A, gamma=0.97)`: implements the value iteration algorithm in the textbook (pg: 105), returns the reached optimal policy $\\pi'$.\n",
                "- `executePolicy(self, pi)`: given $\\pi$, and class attributes such as $S$, $A(S)$ and $V(S)$, executes the policy $\\pi$ while using `self.env` for visualization of the execution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 209,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MDP():\n",
                "    def __init__(self):\n",
                "        self.initial_state = (1, 1, Direction.LEFT.value)\n",
                "        self.final_state = (3, 2, Direction.RIGHT.value)\n",
                "        self.env = ENV()\n",
                "        self.S = self.generateState()\n",
                "        self.A = self.generateActions()\n",
                "        self.V = self.generateV()\n",
                "\n",
                "    def generateState(self):\n",
                "        DIRECTIONS = [Direction.UP.value, Direction.RIGHT.value, Direction.DOWN.value, Direction.LEFT.value] \n",
                "        # Generate all possible states\n",
                "        S = []\n",
                "        for i in range(self.env.W):\n",
                "            for j in range(self.env.H):\n",
                "                for dir in DIRECTIONS:\n",
                "                    S.append((i, j, dir))\n",
                "        S.append('terminal')\n",
                "        return S\n",
                "\n",
                "    def generateActions(self):\n",
                "        # generate all possible actions given every state\n",
                "        A = {}\n",
                "        for s in S:\n",
                "            A[s] = []\n",
                "            if s == 'terminal':\n",
                "                continue\n",
                "            # these actions are possible from all states\n",
                "            A[s].append(Action.turnLeft.value)\n",
                "            A[s].append(Action.turnRight.value)\n",
                "            A[s].append(Action.finish.value)\n",
                "            A[s].append(Action.move.value)\n",
                "        return A\n",
                "        \n",
                "    def generateV(self):\n",
                "        # generate V(s)\n",
                "        V = {}\n",
                "        for s in S:\n",
                "            V[s] = 0\n",
                "        # set terminal state value to 0\n",
                "        V['terminal'] = 0\n",
                "        return V\n",
                "        \n",
                "    @staticmethod\n",
                "    def reward(s, a):\n",
                "        if s == (3, 2, Direction.RIGHT.value) and a == Action.finish.value:\n",
                "            return 100\n",
                "        return -1\n",
                "    @staticmethod\n",
                "    def p(s, a):\n",
                "        r = MDP().reward(s, a)\n",
                "        # get s'\n",
                "        i, j, dir = s\n",
                "        new_i = i\n",
                "        new_j = j\n",
                "        new_dir = dir\n",
                "        terminal = False\n",
                "        WALL1 = (1, 2)\n",
                "        WALL2 = (2, 3)\n",
                "        if a == Action.move.value:\n",
                "            if dir == Direction.UP.value:\n",
                "                new_i -= 1\n",
                "            elif dir == Direction.RIGHT.value:\n",
                "                new_j += 1\n",
                "            elif dir == Direction.DOWN.value:\n",
                "                new_i +=1\n",
                "            elif dir == Direction.LEFT.value:\n",
                "                new_j -=1\n",
                "        elif a == Action.turnRight.value:\n",
                "            if dir == Direction.UP.value:\n",
                "                new_dir = Direction.RIGHT.value\n",
                "            elif dir == Direction.RIGHT.value:\n",
                "                new_dir = Direction.DOWN.value\n",
                "            elif dir == Direction.DOWN.value:\n",
                "                new_dir = Direction.LEFT.value\n",
                "            elif dir == Direction.LEFT.value:\n",
                "                new_dir = Direction.UP.value\n",
                "        elif a == Action.turnLeft.value:\n",
                "            if dir == Direction.UP.value:\n",
                "                new_dir = Direction.LEFT.value\n",
                "            elif dir == Direction.RIGHT.value:\n",
                "                new_dir = Direction.UP.value\n",
                "            elif dir == Direction.DOWN.value:\n",
                "                new_dir = Direction.RIGHT.value\n",
                "            elif dir == Direction.LEFT.value:\n",
                "                new_dir = Direction.DOWN.value\n",
                "        elif a == Action.finish.value:\n",
                "            terminal = True\n",
                "        \n",
                "        out_of_bounds = new_i < 0 or new_i > HEIGHT-1 or new_j <0 or new_j > WIDTH - 1\n",
                "        on_a_wall = (new_i, new_j) == WALL1 or (new_i, new_j) == WALL2\n",
                "        if terminal or out_of_bounds or on_a_wall:\n",
                "            s_ = 'terminal'\n",
                "        else:\n",
                "            s_ = (new_i, new_j, new_dir)\n",
                "        return r, s_\n",
                "\n",
                "    @staticmethod  \n",
                "    def ValueIteration(theta, V, S, A, gamma=0.97):\n",
                "        # theta = 1e-3\n",
                "        Delta = 1e9\n",
                "        num_iterations = 0\n",
                "        while(Delta >= theta):\n",
                "            Delta = 0\n",
                "            for s in S:\n",
                "                v = V[s]\n",
                "                max_a = -1e9\n",
                "                for a in A[s]:\n",
                "                    r, s_ = MDP().p(s, a)\n",
                "                    curr_v = r + gamma*V[s_]\n",
                "                    if curr_v > max_a:\n",
                "                        V[s] = curr_v\n",
                "                        max_a = curr_v\n",
                "                Delta = max(Delta, np.abs(v-V[s]))\n",
                "            num_iterations += 1\n",
                "        # deterministic pollicy pi\n",
                "        pi = {}\n",
                "        for s in S:\n",
                "            max_a = -1e9\n",
                "            a_max_i = 0\n",
                "            # get values all possible actions from s\n",
                "            for i, a in enumerate(A[s]):\n",
                "                r, s_ = MDP().p(s, a)\n",
                "                curr_v = r + gamma*V[s_]\n",
                "                if curr_v > max_a:\n",
                "                    a_max = a\n",
                "                    a_max_i = i\n",
                "                    max_a = curr_v\n",
                "            \n",
                "            pi[s] = a_max_i\n",
                "        print(\"Number of iterations=\", num_iterations)\n",
                "        return pi    \n",
                "    \n",
                "    def executePolicy(self, pi):\n",
                "        s = self.initial_state\n",
                "        action_sequence = []\n",
                "        while(s != 'terminal'):\n",
                "            time.sleep(2)\n",
                "            clear_output(wait=True)\n",
                "            self.env.show(s)\n",
                "            # get action\n",
                "            a = A[s][pi[s]]\n",
                "            r, s_ = MDP().p(s, a)\n",
                "            s = s_\n",
                "            action_sequence.append(a)\n",
                "        return action_sequence\n",
                "        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 210,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of iterations= 8\n",
                        "Total runtime to learn the optimal policy= 0.6183204650878906 (s)\n"
                    ]
                }
            ],
            "source": [
                "# generate MDP() instance, run value iteration\n",
                "mdp = MDP()\n",
                "start = time.time()\n",
                "pi = MDP().ValueIteration(1e-3, mdp.V, mdp.S, mdp.A, gamma=0.9)\n",
                "end = time.time()\n",
                "print(\"Total runtime to learn the optimal policy=\", end-start, \"(s)\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 211,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        ".  .  .  .  \n",
                        "\n",
                        ".  .  \u001b[91m#\u001b[0m  .  \n",
                        "\n",
                        ".  .  .  \u001b[91m#\u001b[0m  \n",
                        "\n",
                        ".  .  \u001b[94m>\u001b[0m  .  \n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# policy execution\n",
                "action_sequence = mdp.executePolicy(pi)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 212,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Actions sequence from optimal policy:\n",
                        "turnLeft\n",
                        "move\n",
                        "move\n",
                        "turnLeft\n",
                        "move\n",
                        "finish\n"
                    ]
                }
            ],
            "source": [
                "# printing action commands\n",
                "actions = ['move', 'turnLeft', 'turnRight', 'finish']\n",
                "print(\"Actions sequence from optimal policy:\")\n",
                "for a in action_sequence:\n",
                "    print(actions[a])"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "62ac1ada3b13c25f2c02d28a927e9b7326b4e70cae37589d8c65c8fbf4379f64"
        },
        "kernelspec": {
            "display_name": "Python 3.9.5 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
